{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7321f7e",
   "metadata": {},
   "source": [
    "### TP2 — Pipeline d'acquisition et transformation de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5ec21",
   "metadata": {},
   "source": [
    "#### Partie 1 : Choix de l'API et exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6dc964",
   "metadata": {},
   "source": [
    "##### 1.1 APIs choisi\n",
    "\n",
    "| **OpenFoodFacts** | Alimentation | [Lien](https://openfoodfacts.github.io/openfoodfacts-server/api/) | ⭐⭐ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d70e80",
   "metadata": {},
   "source": [
    "##### 1.3 Explorer l'API avec l'IA\n",
    "\n",
    "Créez `exploration_api.py` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c88f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from litellm import completion\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def ask_api_assistant(question: str, api_doc: str = \"\") -> str:\n",
    "    \"\"\"Assistant spécialisé dans les APIs.\"\"\"\n",
    "    response = completion(\n",
    "        model=\"gemini/gemini-2.5-flash-lite\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"\"\"Tu es un expert en APIs REST et en data engineering.\n",
    "                Tu aides à comprendre et utiliser des APIs Open Data.\n",
    "                Génère du code Python avec httpx quand on te le demande.\"\"\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"{api_doc}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73525d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_DOC = \"\"\"\n",
    "API OpenFoodFacts :\n",
    "- Base URL: https://world.openfoodfacts.org/api/v2\n",
    "- Endpoint produits: /product/{barcode}.json\n",
    "- Endpoint recherche: /search.json?categories_tags={category}&page_size={n}\n",
    "- Pas d'authentification requise\n",
    "- Rate limit: soyez raisonnables (1 req/sec)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb805ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolument ! Voici comment récupérer les 100 premiers produits de la catégorie 'chocolats' en utilisant l'API OpenFoodFacts avec Python et la librairie `httpx`.\n",
      "\n",
      "```python\n",
      "import httpx\n",
      "import time\n",
      "\n",
      "# Configuration de l'API\n",
      "BASE_URL = \"https://world.openfoodfacts.org/api/v2\"\n",
      "SEARCH_ENDPOINT = \"/search.json\"\n",
      "\n",
      "# Paramètres de la recherche\n",
      "CATEGORY = \"chocolats\"\n",
      "PAGE_SIZE = 100\n",
      "REQUEST_DELAY = 1  # Délai entre les requêtes en secondes pour respecter le rate limit\n",
      "\n",
      "# Construit l'URL complète pour la recherche\n",
      "url = f\"{BASE_URL}{SEARCH_ENDPOINT}\"\n",
      "params = {\n",
      "    \"categories_tags\": CATEGORY,\n",
      "    \"page_size\": PAGE_SIZE\n",
      "}\n",
      "\n",
      "print(f\"Récupération des {PAGE_SIZE} premiers produits de la catégorie '{CATEGORY}'...\")\n",
      "\n",
      "try:\n",
      "    # Effectue la requête GET\n",
      "    response = httpx.get(url, params=params)\n",
      "    response.raise_for_status()  # Lève une exception pour les codes d'erreur HTTP (4xx, 5xx)\n",
      "\n",
      "    # Parse la réponse JSON\n",
      "    data = response.json()\n",
      "\n",
      "    # Vérifie si des produits ont été trouvés\n",
      "    if data and \"products\" in data and data[\"products\"]:\n",
      "        products = data[\"products\"]\n",
      "        print(f\"Trouvé {len(products)} produits.\")\n",
      "\n",
      "        # Affiche quelques informations sur les produits (par exemple, le nom et le code-barres)\n",
      "        for i, product in enumerate(products):\n",
      "            product_name = product.get(\"product_name\", \"Nom non disponible\")\n",
      "            barcode = product.get(\"code\", \"Code-barres non disponible\")\n",
      "            print(f\"{i+1}. Nom: {product_name}, Code-barres: {barcode}\")\n",
      "\n",
      "    else:\n",
      "        print(\"Aucun produit trouvé pour cette catégorie.\")\n",
      "\n",
      "except httpx.HTTPStatusError as e:\n",
      "    print(f\"Erreur HTTP: {e.response.status_code} - {e.response.text}\")\n",
      "except httpx.RequestError as e:\n",
      "    print(f\"Erreur de requête: Impossible de se connecter à l'API. Détails: {e}\")\n",
      "except Exception as e:\n",
      "    print(f\"Une erreur inattendue est survenue: {e}\")\n",
      "\n",
      "print(\"Opération terminée.\")\n",
      "```\n",
      "\n",
      "**Explication du code :**\n",
      "\n",
      "1.  **Importation des librairies :**\n",
      "    *   `httpx` : Une librairie moderne et asynchrone pour faire des requêtes HTTP.\n",
      "    *   `time` : Utilisé ici pour implémenter une pause si l'on devait faire plusieurs appels consécutifs (bien que pour une seule requête, ce ne soit pas strictement nécessaire, c'est une bonne pratique à garder en tête pour le rate limit).\n",
      "\n",
      "2.  **Configuration :**\n",
      "    *   `BASE_URL` : L'adresse de base de l'API.\n",
      "    *   `SEARCH_ENDPOINT` : Le chemin spécifique pour les recherches.\n",
      "    *   `CATEGORY` : Le tag de catégorie que nous recherchons ('chocolats').\n",
      "    *   `PAGE_SIZE` : Le nombre de résultats que nous voulons par page (ici, 100).\n",
      "    *   `REQUEST_DELAY` : Le temps d'attente en secondes entre les requêtes. Bien que nous n'effectuions qu'une seule requête ici, c'est une bonne habitude à prendre pour les appels multiples afin de ne pas surcharger le serveur.\n",
      "\n",
      "3.  **Construction de l'URL :**\n",
      "    *   L'URL complète est formée en combinant `BASE_URL` et `SEARCH_ENDPOINT`.\n",
      "    *   Les `params` sont passés sous forme de dictionnaire. `httpx` s'occupera de les formater correctement dans l'URL (`?categories_tags=chocolats&page_size=100`).\n",
      "\n",
      "4.  **Exécution de la requête :**\n",
      "    *   `httpx.get(url, params=params)` : Envoie une requête GET à l'URL spécifiée avec les paramètres définis.\n",
      "    *   `response.raise_for_status()` : Cette méthode est très utile. Si la réponse du serveur indique une erreur (comme 404 Non trouvé, 429 Trop de requêtes, 500 Erreur serveur), elle lèvera une exception `httpx.HTTPStatusError`.\n",
      "\n",
      "5.  **Traitement de la réponse :**\n",
      "    *   `data = response.json()` : Convertit la réponse JSON du serveur en un dictionnaire Python.\n",
      "    *   La structure de la réponse de l'API OpenFoodFacts contient une clé `\"products\"` qui est une liste des produits trouvés. Le code vérifie si cette clé existe et si la liste n'est pas vide.\n",
      "    *   Une boucle `for` itère sur la liste `products` et affiche le nom et le code-barres de chaque produit. `.get(\"key\", \"default_value\")` est utilisé pour éviter les erreurs si une clé n'est pas présente pour un produit donné.\n",
      "\n",
      "6.  **Gestion des erreurs :**\n",
      "    *   Le bloc `try...except` permet de capturer différentes erreurs potentielles :\n",
      "        *   `httpx.HTTPStatusError` : Pour les erreurs renvoyées par l'API (ex: mauvaise requête, problème serveur).\n",
      "        *   `httpx.RequestError` : Pour les problèmes de connexion réseau (ex: pas d'internet, serveur indisponible).\n",
      "        *   `Exception` : Pour toute autre erreur imprévue.\n",
      "\n",
      "**Pour exécuter ce code :**\n",
      "\n",
      "1.  Assurez-vous d'avoir installé la librairie `httpx` :\n",
      "    ```bash\n",
      "    pip install httpx\n",
      "    ```\n",
      "2.  Copiez-collez le code dans un fichier Python (par exemple, `openfoodfacts_search.py`).\n",
      "3.  Exécutez-le depuis votre terminal :\n",
      "    ```bash\n",
      "    python openfoodfacts_search.py\n",
      "    ```\n"
     ]
    }
   ],
   "source": [
    "# Demander à l'IA comment utiliser l'API\n",
    "print(ask_api_assistant(\n",
    "    \"Comment récupérer les 100 premiers produits de la catégorie 'chocolats' ?\",\n",
    "    API_DOC\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c6a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête vers : https://world.openfoodfacts.org/api/v2/search.json?categories_tags=chocolats&page_size=100\n",
      "\n",
      "--- Les 6 premiers produits de la catégorie 'chocolats' ---\n",
      "1. Nom: Dolca chocolate negro, Code-barres: 7613036723671\n",
      "2. Nom: Haselnuss Schokolade, Code-barres: 4099200057323\n",
      "3. Nom: faire Schokolade 85% Kakao, Code-barres: 4006040412892\n",
      "4. Nom: , Code-barres: 7627535092121\n",
      "5. Nom: Nocciolata Dark Chocolate With Whole Hazelnuts, Dark, Code-barres: 8002996303129\n",
      "6. Nom: Pistole chocolat noir, Code-barres: 3760377561241\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import time\n",
    "\n",
    "# Configuration de l'API OpenFoodFacts\n",
    "BASE_URL = \"https://world.openfoodfacts.org/api/v2\"\n",
    "SEARCH_ENDPOINT = \"/search.json\"\n",
    "\n",
    "# Paramètres de la requête\n",
    "category = \"chocolats\"\n",
    "page_size = 100\n",
    "# On pourrait aussi spécifier une page si on voulait une autre tranche de résultats\n",
    "# page = 1\n",
    "\n",
    "# Construction de l'URL de recherche\n",
    "# url = f\"{BASE_URL}{SEARCH_ENDPOINT}?categories_tags={category}&page_size={page_size}&page={page}\"\n",
    "url = f\"{BASE_URL}{SEARCH_ENDPOINT}?categories_tags={category}&page_size={page_size}\"\n",
    "\n",
    "print(f\"Requête vers : {url}\")\n",
    "\n",
    "try:\n",
    "    # Utilisation de httpx avec un timeout plus long\n",
    "    with httpx.Client(timeout=60.0) as client:\n",
    "        response = client.get(url)\n",
    "\n",
    "    # Vérifier si la requête a réussi\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Charger la réponse JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Extraire la liste des produits\n",
    "    products = data.get(\"products\", [])\n",
    "\n",
    "    if products:\n",
    "        print(f\"\\n--- Les {len(products)} premiers produits de la catégorie '{category}' ---\")\n",
    "        for i, product in enumerate(products):\n",
    "            product_name = product.get(\"product_name\", \"Nom non disponible\")\n",
    "            barcode = product.get(\"code\", \"Code-barres non disponible\")\n",
    "            print(f\"{i+1}. Nom: {product_name}, Code-barres: {barcode}\")\n",
    "    else:\n",
    "        print(f\"Aucun produit trouvé pour la catégorie '{category}'.\")\n",
    "\n",
    "except httpx.HTTPStatusError as e:\n",
    "    print(f\"Erreur HTTP : {e.response.status_code}\")\n",
    "except httpx.RequestError as e:\n",
    "    print(f\"Erreur réseau : {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur inattendue : {e}\")\n",
    "\n",
    "# Respect du rate limit (1 requête par seconde)\n",
    "# Dans cet exemple, nous n'avons qu'une seule requête, donc ce n'est pas strictement\n",
    "# nécessaire. Mais si vous faisiez plusieurs requêtes successives dans une boucle,\n",
    "# il faudrait ajouter ceci entre chaque requête.\n",
    "# time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564c3d8",
   "metadata": {},
   "source": [
    "**Explication du code :**\n",
    "\n",
    "1.  **Importation des librairies :**\n",
    "    *   `httpx` : Une librairie moderne pour effectuer des requêtes HTTP asynchrones et synchrones. Elle est recommandée pour sa simplicité et ses performances.\n",
    "    *   `time` : Utilisé ici pour illustrer le respect du rate limit (même si ce n'est pas strictement nécessaire pour une seule requête).\n",
    "\n",
    "2.  **Configuration :**\n",
    "    *   `BASE_URL` : L'adresse de base de l'API OpenFoodFacts.\n",
    "    *   `SEARCH_ENDPOINT` : Le chemin spécifique pour effectuer des recherches.\n",
    "\n",
    "3.  **Paramètres de la requête :**\n",
    "    *   `category` : La catégorie que nous recherchons ('chocolats').\n",
    "    *   `page_size` : Le nombre de résultats que nous voulons par page (ici, 100).\n",
    "\n",
    "4.  **Construction de l'URL :**\n",
    "    *   L'URL complète est créée en combinant la base URL, le endpoint de recherche et les paramètres (`categories_tags`, `page_size`).\n",
    "\n",
    "5.  **Exécution de la requête :**\n",
    "    *   `httpx.get(url)` : Envoie une requête HTTP GET à l'URL construite.\n",
    "    *   `response.raise_for_status()` : Vérifie si le code de statut de la réponse indique un succès (par exemple, 200 OK). Si ce n'est pas le cas (par exemple, 404 Not Found, 500 Internal Server Error), une exception `httpx.HTTPStatusError` sera levée.\n",
    "    *   `response.json()` : Parse la réponse JSON du serveur en un dictionnaire Python.\n",
    "\n",
    "6.  **Traitement des résultats :**\n",
    "    *   `data.get(\"products\", [])` : Extrait la liste des produits du dictionnaire `data`. Si la clé `\"products\"` n'existe pas, elle retourne une liste vide pour éviter une erreur.\n",
    "    *   La boucle `for` itère sur chaque produit trouvé et affiche son nom (`product_name`) et son code-barres (`code`). Des messages par défaut sont utilisés si ces informations sont manquantes.\n",
    "\n",
    "7.  **Gestion des erreurs :**\n",
    "    *   Les blocs `try...except` permettent de capturer différentes erreurs qui pourraient survenir :\n",
    "        *   `httpx.HTTPStatusError` : Pour les erreurs liées au statut de la réponse HTTP.\n",
    "        *   `httpx.RequestError` : Pour les erreurs de connexion réseau ou d'autres problèmes liés à la requête elle-même.\n",
    "        *   `Exception` : Pour toute autre erreur inattendue.\n",
    "\n",
    "8.  **Respect du rate limit :**\n",
    "    *   Bien que non exécuté dans cet exemple, `time.sleep(1)` serait utilisé après chaque requête si vous effectuez plusieurs appels à l'API de manière consécutive dans une boucle, pour respecter la limite de 1 requête par seconde.\n",
    "\n",
    "Ce script donne les informations sur les 100 premiers produits trouvés dans la catégorie 'chocolats' par l'API OpenFoodFacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ad928",
   "metadata": {},
   "source": [
    "##### 1.4 Premier appel API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a9c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de produits : 6\n",
      "- Dolca chocolate negro (Dolca,Nestlé)\n",
      "- Haselnuss Schokolade (Choceur)\n",
      "- faire Schokolade 85% Kakao (Rapunzel)\n",
      "-  (Coop,Karma)\n",
      "- Nocciolata Dark Chocolate With Whole Hazelnuts, Dark (Venchi)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test de l'API.\"\"\"\n",
    "import httpx\n",
    "\n",
    "# Exemple OpenFoodFacts - adapter selon votre API\n",
    "BASE_URL = \"https://world.openfoodfacts.org/api/v2\"\n",
    "\n",
    "def test_api():\n",
    "    \"\"\"Test un appel simple à l'API.\"\"\"\n",
    "    response = httpx.get(\n",
    "        f\"{BASE_URL}/search\",\n",
    "        params={\n",
    "            \"categories_tags\": \"chocolats\",\n",
    "            \"page_size\": 5,\n",
    "            \"fields\": \"code,product_name,brands,nutriscore_grade\"\n",
    "        },\n",
    "        timeout=30\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    print(f\"Nombre de produits : {data.get('count', 'N/A')}\")\n",
    "    for product in data.get(\"products\", []):\n",
    "        print(f\"- {product.get('product_name', 'N/A')} ({product.get('brands', 'N/A')})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414f463",
   "metadata": {},
   "source": [
    "6 produits ont été récupérés. Parmi ceux-ci, on trouve des chocolats de marques variées, allant des grandes marques internationales comme Nestlé et Venchi, à des marques bio ou locales comme Rapunzel et Choceur. Certains produits contiennent des informations complètes sur le nom et la marque, tandis qu’un produit semble avoir une marque renseignée mais pas de nom ((Coop,Karma)). Ce jeu de données initial met en évidence la nécessité d’un nettoyage pour gérer les valeurs manquantes et standardiser les noms et marques avant toute analyse ou transformation ultérieure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d36afb",
   "metadata": {},
   "source": [
    "### Partie 2 : Construction du pipeline d'acquisition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275012ce",
   "metadata": {},
   "source": [
    "##### 2.4 Exercice : Adapter à votre API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c424b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolument ! Voici le code Python adapté du `fetcher.py` pour interroger l'API OpenFoodFacts et récupérer les champs spécifiés, tout en conservant la logique de retry et de pagination.\n",
      "\n",
      "```python\n",
      "import time\n",
      "import httpx\n",
      "from typing import Dict, Any, List, Optional\n",
      "\n",
      "# Configuration de l'API OpenFoodFacts\n",
      "API_URL = \"https://fr.openfoodfacts.org/cgi/search.pl\"\n",
      "# Champs que nous souhaitons récupérer pour chaque produit\n",
      "FIELDS = [\n",
      "    \"code\",\n",
      "    \"product_name\",\n",
      "    \"brands\",\n",
      "    \"categories\",\n",
      "    \"nutriscore_grade\",\n",
      "    \"energy_100g\",\n",
      "    \"fat_100g\",\n",
      "    \"sugars_100g\",\n",
      "    \"salt_100g\",\n",
      "    \"proteins_100g\",\n",
      "]\n",
      "\n",
      "# Paramètres de recherche initiaux (vous pouvez les modifier)\n",
      "# Ici, on cherche des produits dont le nom contient \"pomme\" en France\n",
      "SEARCH_PARAMS = {\n",
      "    \"search_terms\": \"pomme\",\n",
      "    \"search_simple\": 1,\n",
      "    \"action\": \"process\",\n",
      "    \"country\": \"France\",\n",
      "    \"page_size\": 24,  # Nombre de résultats par page\n",
      "    \"fields\": \",\".join(FIELDS),\n",
      "    \"json\": 1,\n",
      "}\n",
      "\n",
      "# Paramètres de retry\n",
      "MAX_RETRIES = 5\n",
      "RETRY_DELAY_SECONDS = 5\n",
      "\n",
      "async def fetch_data(\n",
      "    client: httpx.AsyncClient, params: Dict[str, Any]\n",
      ") -> Optional[Dict[str, Any]]:\n",
      "    \"\"\"\n",
      "    Récupère des données depuis l'API OpenFoodFacts avec gestion des tentatives.\n",
      "\n",
      "    Args:\n",
      "        client: Un client httpx.AsyncClient.\n",
      "        params: Un dictionnaire contenant les paramètres de la requête.\n",
      "\n",
      "    Returns:\n",
      "        Un dictionnaire contenant les données JSON de la réponse,\n",
      "        ou None en cas d'échec après plusieurs tentatives.\n",
      "    \"\"\"\n",
      "    for attempt in range(MAX_RETRIES):\n",
      "        try:\n",
      "            response = await client.get(API_URL, params=params)\n",
      "            response.raise_for_status()  # Lève une exception pour les codes d'erreur HTTP (4xx, 5xx)\n",
      "            return response.json()\n",
      "        except httpx.HTTPStatusError as e:\n",
      "            print(f\"Erreur HTTP lors de la tentative {attempt + 1}/{MAX_RETRIES}: {e}\")\n",
      "            if attempt < MAX_RETRIES - 1:\n",
      "                print(f\"Nouvelle tentative dans {RETRY_DELAY_SECONDS} secondes...\")\n",
      "                await time.sleep(RETRY_DELAY_SECONDS)\n",
      "            else:\n",
      "                print(\"Nombre maximum de tentatives atteint. Impossible de récupérer les données.\")\n",
      "                return None\n",
      "        except httpx.RequestError as e:\n",
      "            print(f\"Erreur de requête lors de la tentative {attempt + 1}/{MAX_RETRIES}: {e}\")\n",
      "            if attempt < MAX_RETRIES - 1:\n",
      "                print(f\"Nouvelle tentative dans {RETRY_DELAY_SECONDS} secondes...\")\n",
      "                await time.sleep(RETRY_DELAY_SECONDS)\n",
      "            else:\n",
      "                print(\"Nombre maximum de tentatives atteint. Impossible de récupérer les données.\")\n",
      "                return None\n",
      "    return None\n",
      "\n",
      "async def fetch_all_products(initial_params: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"\n",
      "    Récupère tous les produits correspondant aux critères de recherche\n",
      "    en gérant la pagination.\n",
      "\n",
      "    Args:\n",
      "        initial_params: Les paramètres initiaux de la recherche.\n",
      "\n",
      "    Returns:\n",
      "        Une liste de dictionnaires, où chaque dictionnaire représente un produit.\n",
      "    \"\"\"\n",
      "    all_products = []\n",
      "    current_page = 1\n",
      "    total_pages = 1  # Initialisé à 1 pour entrer dans la boucle\n",
      "\n",
      "    async with httpx.AsyncClient() as client:\n",
      "        while current_page <= total_pages:\n",
      "            print(f\"Récupération de la page {current_page}...\")\n",
      "            params = initial_params.copy()\n",
      "            params[\"page\"] = current_page\n",
      "\n",
      "            data = await fetch_data(client, params)\n",
      "\n",
      "            if data and \"products\" in data:\n",
      "                products = data[\"products\"]\n",
      "                all_products.extend(products)\n",
      "\n",
      "                # Mise à jour du nombre total de pages si ce n'est pas déjà fait\n",
      "                if current_page == 1:\n",
      "                    # OpenFoodFacts retourne le nombre total de résultats, on calcule les pages\n",
      "                    total_results = data.get(\"count\", 0)\n",
      "                    page_size = params.get(\"page_size\", 24)\n",
      "                    total_pages = (total_results + page_size - 1) // page_size\n",
      "                    print(f\"Nombre total de produits trouvés : {total_results}\")\n",
      "                    print(f\"Nombre total de pages à récupérer : {total_pages}\")\n",
      "\n",
      "                current_page += 1\n",
      "            else:\n",
      "                print(\"Aucun produit trouvé sur cette page ou erreur lors de la récupération.\")\n",
      "                break # Arrête la boucle s'il n'y a pas de produits ou une erreur\n",
      "\n",
      "    print(f\"Récupération terminée. {len(all_products)} produits ont été collectés.\")\n",
      "    return all_products\n",
      "\n",
      "async def main():\n",
      "    \"\"\"Fonction principale pour exécuter le processus de récupération.\"\"\"\n",
      "    products = await fetch_all_products(SEARCH_PARAMS)\n",
      "\n",
      "    if products:\n",
      "        print(f\"\\nAffichage des 5 premiers produits récupérés (avec les champs demandés) :\")\n",
      "        for i, product in enumerate(products[:5]):\n",
      "            print(f\"--- Produit {i+1} ---\")\n",
      "            for field in FIELDS:\n",
      "                # Affiche le champ s'il existe, sinon indique qu'il est manquant\n",
      "                value = product.get(field, \"N/A\")\n",
      "                print(f\"  {field}: {value}\")\n",
      "        print(\"\\n...\") # Indique qu'il y a potentiellement plus de produits\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import asyncio\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "### Explication du code :\n",
      "\n",
      "1.  **`API_URL` et `FIELDS`**:\n",
      "    *   `API_URL` pointe vers le point d'entrée de la recherche de l'API OpenFoodFacts.\n",
      "    *   `FIELDS` est une liste contenant tous les noms des champs que vous souhaitez extraire. Ces noms correspondent aux clés dans les objets JSON retournés par l'API.\n",
      "\n",
      "2.  **`SEARCH_PARAMS`**:\n",
      "    *   C'est un dictionnaire qui définit les critères de votre recherche.\n",
      "        *   `search_terms`: Les termes à rechercher (ici, \"pomme\").\n",
      "        *   `search_simple`: 1 pour une recherche simple.\n",
      "        *   `action`: \"process\" pour exécuter la recherche.\n",
      "        *   `country`: \"France\" pour cibler les produits français.\n",
      "        *   `page_size`: Le nombre de produits à récupérer par requête. J'ai mis 24, une valeur courante.\n",
      "        *   `fields`: On joint les éléments de la liste `FIELDS` avec une virgule pour former la chaîne attendue par l'API (ex: `\"code,product_name,brands\"`).\n",
      "        *   `json`: 1 pour s'assurer que la réponse est au format JSON.\n",
      "\n",
      "3.  **`MAX_RETRIES` et `RETRY_DELAY_SECONDS`**:\n",
      "    *   Ces constantes définissent la stratégie de relance en cas d'erreur réseau ou HTTP.\n",
      "\n",
      "4.  **`fetch_data(client, params)`**:\n",
      "    *   Cette fonction est responsable d'une seule requête à l'API.\n",
      "    *   Elle boucle jusqu'à `MAX_RETRIES` fois.\n",
      "    *   Elle utilise `httpx.AsyncClient` pour effectuer des requêtes asynchrones.\n",
      "    *   `response.raise_for_status()` est crucial : il lèvera une `httpx.HTTPStatusError` pour les codes d'état HTTP comme 404, 500, etc., ce qui déclenchera la logique de retry.\n",
      "    *   `httpx.RequestError` attrape les erreurs de connexion, timeouts, etc.\n",
      "    *   Si une erreur se produit et que le nombre de tentatives n'est pas atteint, elle attend `RETRY_DELAY_SECONDS` avant de réessayer.\n",
      "    *   Si toutes les tentatives échouent, elle retourne `None`.\n",
      "\n",
      "5.  **`fetch_all_products(initial_params)`**:\n",
      "    *   C'est la fonction principale de récupération.\n",
      "    *   Elle initialise une liste vide `all_products`.\n",
      "    *   Elle utilise une boucle `while` pour gérer la pagination. La boucle continue tant que `current_page` est inférieure ou égale à `total_pages`.\n",
      "    *   Dans chaque itération :\n",
      "        *   Elle prépare les paramètres de la requête en copiant `initial_params` et en ajoutant le paramètre `page` courant.\n",
      "        *   Elle appelle `fetch_data` pour obtenir les données de la page actuelle.\n",
      "        *   Si des données sont reçues et qu'elles contiennent la clé `\"products\"`:\n",
      "            *   Elle ajoute les produits de la page à `all_products`.\n",
      "            *   Lors de la première page (`current_page == 1`), elle extrait le nombre total de résultats (`\"count\"`) de la réponse. Elle calcule ensuite `total_pages` en divisant le nombre total de résultats par la taille de la page (`page_size`), en arrondissant vers le haut.\n",
      "            *   Elle incrémente `current_page` pour passer à la page suivante.\n",
      "        *   Si aucun produit n'est trouvé ou s'il y a une erreur, la boucle est interrompue.\n",
      "    *   Elle retourne la liste complète `all_products`.\n",
      "\n",
      "6.  **`main()`**:\n",
      "    *   C'est le point d'entrée de l'exécution asynchrone.\n",
      "    *   Elle appelle `fetch_all_products` avec les `SEARCH_PARAMS` définis.\n",
      "    *   Si des produits sont récupérés, elle affiche les 5 premiers avec les champs spécifiés pour montrer le résultat.\n",
      "\n",
      "7.  **`if __name__ == \"__main__\":`**:\n",
      "    *   Ce bloc standard assure que `asyncio.run(main())` est exécuté uniquement lorsque le script est lancé directement.\n",
      "\n",
      "### Comment l'utiliser :\n",
      "\n",
      "1.  **Sauvegardez** ce code dans un fichier Python (par exemple, `openfoodfacts_fetcher.py`).\n",
      "2.  **Installez `httpx`** si ce n'est pas déjà fait :\n",
      "    ```bash\n",
      "    pip install httpx\n",
      "    ```\n",
      "3.  **Exécutez le script** depuis votre terminal :\n",
      "    ```bash\n",
      "    python openfoodfacts_fetcher.py\n",
      "    ```\n",
      "\n",
      "Vous pouvez modifier les `SEARCH_PARAMS` pour changer les critères de recherche (par exemple, changer `\"pomme\"` pour `\"lait\"`, ou ajouter d'autres paramètres comme `\"brands\"`, `\"categories\"` directement dans `SEARCH_PARAMS` pour affiner la recherche).\n"
     ]
    }
   ],
   "source": [
    "# Dans votre notebook ou script\n",
    "prompt = f\"\"\"\n",
    "J'ai cette API : OpenFoodFacts** | Alimentation | \n",
    "Documentation : https://openfoodfacts.github.io/openfoodfacts-server/api/\n",
    "\n",
    "Adapte le code de fetcher.py pour récupérer tous les produits contenant les champs code, product_name, brands, categories, nutriscore_grade, energy_100g, fat_100g, sugars_100g, salt_100g, proteins_100g\n",
    "\n",
    "Garde la même structure avec retry et pagination.\n",
    "\"\"\"\n",
    "\n",
    "print(ask_api_assistant(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff726a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 20:31:31,108 - INFO - Début de la récupération des produits. Terme de recherche: 'chocolat'\n",
      "2025-12-16 20:31:31,109 - INFO - Requête pour la page 1...\n",
      "2025-12-16 20:31:36,001 - INFO - Page 1 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:36,003 - INFO - Requête pour la page 2...\n",
      "2025-12-16 20:31:37,087 - INFO - Page 2 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:37,087 - INFO - Requête pour la page 3...\n",
      "2025-12-16 20:31:38,167 - INFO - Page 3 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:38,168 - INFO - Requête pour la page 4...\n",
      "2025-12-16 20:31:39,803 - INFO - Page 4 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:39,804 - INFO - Requête pour la page 5...\n",
      "2025-12-16 20:31:41,068 - INFO - Page 5 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:41,069 - INFO - Requête pour la page 6...\n",
      "2025-12-16 20:31:42,488 - INFO - Page 6 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:42,488 - INFO - Requête pour la page 7...\n",
      "2025-12-16 20:31:43,595 - INFO - Page 7 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:43,596 - INFO - Requête pour la page 8...\n",
      "2025-12-16 20:31:44,928 - INFO - Page 8 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:44,929 - INFO - Requête pour la page 9...\n",
      "2025-12-16 20:31:45,944 - INFO - Page 9 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:45,945 - INFO - Requête pour la page 10...\n",
      "2025-12-16 20:31:47,020 - INFO - Page 10 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:47,021 - INFO - Requête pour la page 11...\n",
      "2025-12-16 20:31:48,586 - INFO - Page 11 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:48,587 - INFO - Requête pour la page 12...\n",
      "2025-12-16 20:31:50,334 - INFO - Page 12 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:50,334 - INFO - Requête pour la page 13...\n",
      "2025-12-16 20:31:51,766 - INFO - Page 13 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:51,767 - INFO - Requête pour la page 14...\n",
      "2025-12-16 20:31:52,897 - INFO - Page 14 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:52,898 - INFO - Requête pour la page 15...\n",
      "2025-12-16 20:31:53,780 - INFO - Page 15 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:53,781 - INFO - Requête pour la page 16...\n",
      "2025-12-16 20:31:54,953 - INFO - Page 16 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:54,955 - INFO - Requête pour la page 17...\n",
      "2025-12-16 20:31:56,085 - INFO - Page 17 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:56,086 - INFO - Requête pour la page 18...\n",
      "2025-12-16 20:31:57,025 - INFO - Page 18 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:57,026 - INFO - Requête pour la page 19...\n",
      "2025-12-16 20:31:57,888 - INFO - Page 19 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:57,889 - INFO - Requête pour la page 20...\n",
      "2025-12-16 20:31:58,820 - INFO - Page 20 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:58,821 - INFO - Requête pour la page 21...\n",
      "2025-12-16 20:31:59,773 - INFO - Page 21 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:31:59,774 - INFO - Requête pour la page 22...\n",
      "2025-12-16 20:32:00,890 - INFO - Page 22 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:00,891 - INFO - Requête pour la page 23...\n",
      "2025-12-16 20:32:01,834 - INFO - Page 23 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:01,835 - INFO - Requête pour la page 24...\n",
      "2025-12-16 20:32:02,756 - INFO - Page 24 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:02,756 - INFO - Requête pour la page 25...\n",
      "2025-12-16 20:32:04,087 - INFO - Page 25 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:04,088 - INFO - Requête pour la page 26...\n",
      "2025-12-16 20:32:05,211 - INFO - Page 26 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:05,212 - INFO - Requête pour la page 27...\n",
      "2025-12-16 20:32:06,303 - INFO - Page 27 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:06,303 - INFO - Requête pour la page 28...\n",
      "2025-12-16 20:32:07,409 - INFO - Page 28 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:07,411 - INFO - Requête pour la page 29...\n",
      "2025-12-16 20:32:08,756 - INFO - Page 29 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:08,756 - INFO - Requête pour la page 30...\n",
      "2025-12-16 20:32:10,193 - INFO - Page 30 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:10,194 - INFO - Requête pour la page 31...\n",
      "2025-12-16 20:32:11,949 - INFO - Page 31 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:11,949 - INFO - Requête pour la page 32...\n",
      "2025-12-16 20:32:13,470 - INFO - Page 32 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:13,471 - INFO - Requête pour la page 33...\n",
      "2025-12-16 20:32:14,809 - INFO - Page 33 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:14,810 - INFO - Requête pour la page 34...\n",
      "2025-12-16 20:32:16,293 - INFO - Page 34 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:16,294 - INFO - Requête pour la page 35...\n",
      "2025-12-16 20:32:17,822 - INFO - Page 35 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:17,822 - INFO - Requête pour la page 36...\n",
      "2025-12-16 20:32:20,808 - INFO - Page 36 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:20,809 - INFO - Requête pour la page 37...\n",
      "2025-12-16 20:32:23,898 - INFO - Page 37 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:23,898 - INFO - Requête pour la page 38...\n",
      "2025-12-16 20:32:27,940 - INFO - Page 38 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:27,940 - INFO - Requête pour la page 39...\n",
      "2025-12-16 20:32:30,485 - INFO - Page 39 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:30,486 - INFO - Requête pour la page 40...\n",
      "2025-12-16 20:32:34,256 - INFO - Page 40 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:34,257 - INFO - Requête pour la page 41...\n",
      "2025-12-16 20:32:37,849 - INFO - Page 41 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:37,849 - INFO - Requête pour la page 42...\n",
      "2025-12-16 20:32:39,926 - INFO - Page 42 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:39,926 - INFO - Requête pour la page 43...\n",
      "2025-12-16 20:32:43,132 - INFO - Page 43 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:43,132 - INFO - Requête pour la page 44...\n",
      "2025-12-16 20:32:46,337 - INFO - Page 44 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:46,338 - INFO - Requête pour la page 45...\n",
      "2025-12-16 20:32:51,296 - INFO - Page 45 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:51,297 - INFO - Requête pour la page 46...\n",
      "2025-12-16 20:32:58,806 - INFO - Page 46 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:32:58,807 - INFO - Requête pour la page 47...\n",
      "2025-12-16 20:33:07,015 - INFO - Page 47 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:33:07,016 - INFO - Requête pour la page 48...\n",
      "2025-12-16 20:33:15,993 - INFO - Page 48 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:33:15,994 - INFO - Requête pour la page 49...\n",
      "2025-12-16 20:33:25,199 - INFO - Page 49 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:33:25,200 - INFO - Requête pour la page 50...\n",
      "2025-12-16 20:33:32,788 - INFO - Page 50 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:33:32,788 - INFO - Requête pour la page 51...\n",
      "2025-12-16 20:33:40,013 - INFO - Page 51 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:33:40,013 - INFO - Requête pour la page 52...\n",
      "2025-12-16 20:33:48,605 - INFO - Page 52 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:33:48,605 - INFO - Requête pour la page 53...\n",
      "2025-12-16 20:34:00,429 - INFO - Page 53 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:34:00,430 - INFO - Requête pour la page 54...\n",
      "2025-12-16 20:34:15,848 - INFO - Page 54 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:34:15,848 - INFO - Requête pour la page 55...\n",
      "2025-12-16 20:34:30,771 - INFO - Page 55 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:34:30,772 - INFO - Requête pour la page 56...\n",
      "2025-12-16 20:34:45,116 - INFO - Page 56 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:34:45,116 - INFO - Requête pour la page 57...\n",
      "2025-12-16 20:35:00,044 - INFO - Page 57 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:00,045 - INFO - Requête pour la page 58...\n",
      "2025-12-16 20:35:16,093 - INFO - Page 58 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:16,094 - INFO - Requête pour la page 59...\n",
      "2025-12-16 20:35:34,332 - INFO - Page 59 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:34,333 - INFO - Requête pour la page 60...\n",
      "2025-12-16 20:35:46,761 - INFO - Page 60 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:46,761 - INFO - Requête pour la page 61...\n",
      "2025-12-16 20:35:51,550 - INFO - Page 61 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:51,550 - INFO - Requête pour la page 62...\n",
      "2025-12-16 20:35:53,213 - INFO - Page 62 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:53,214 - INFO - Requête pour la page 63...\n",
      "2025-12-16 20:35:54,612 - INFO - Page 63 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:54,613 - INFO - Requête pour la page 64...\n",
      "2025-12-16 20:35:56,080 - INFO - Page 64 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:56,081 - INFO - Requête pour la page 65...\n",
      "2025-12-16 20:35:57,425 - INFO - Page 65 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:57,426 - INFO - Requête pour la page 66...\n",
      "2025-12-16 20:35:58,555 - INFO - Page 66 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:35:58,556 - INFO - Requête pour la page 67...\n",
      "2025-12-16 20:36:00,251 - INFO - Page 67 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:00,251 - INFO - Requête pour la page 68...\n",
      "2025-12-16 20:36:01,526 - INFO - Page 68 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:01,530 - INFO - Requête pour la page 69...\n",
      "2025-12-16 20:36:03,086 - INFO - Page 69 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:03,086 - INFO - Requête pour la page 70...\n",
      "2025-12-16 20:36:05,001 - INFO - Page 70 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:05,002 - INFO - Requête pour la page 71...\n",
      "2025-12-16 20:36:06,129 - INFO - Page 71 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:06,130 - INFO - Requête pour la page 72...\n",
      "2025-12-16 20:36:07,263 - INFO - Page 72 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:07,263 - INFO - Requête pour la page 73...\n",
      "2025-12-16 20:36:09,209 - INFO - Page 73 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:09,210 - INFO - Requête pour la page 74...\n",
      "2025-12-16 20:36:10,650 - INFO - Page 74 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:10,652 - INFO - Requête pour la page 75...\n",
      "2025-12-16 20:36:12,421 - INFO - Page 75 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:12,422 - INFO - Requête pour la page 76...\n",
      "2025-12-16 20:36:14,078 - INFO - Page 76 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:14,079 - INFO - Requête pour la page 77...\n",
      "2025-12-16 20:36:15,782 - INFO - Page 77 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:15,783 - INFO - Requête pour la page 78...\n",
      "2025-12-16 20:36:17,069 - INFO - Page 78 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:17,070 - INFO - Requête pour la page 79...\n",
      "2025-12-16 20:36:18,250 - INFO - Page 79 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:18,251 - INFO - Requête pour la page 80...\n",
      "2025-12-16 20:36:19,591 - INFO - Page 80 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:19,591 - INFO - Requête pour la page 81...\n",
      "2025-12-16 20:36:20,882 - INFO - Page 81 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:20,882 - INFO - Requête pour la page 82...\n",
      "2025-12-16 20:36:22,105 - INFO - Page 82 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:22,105 - INFO - Requête pour la page 83...\n",
      "2025-12-16 20:36:25,027 - INFO - Page 83 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:25,028 - INFO - Requête pour la page 84...\n",
      "2025-12-16 20:36:28,555 - INFO - Page 84 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:28,556 - INFO - Requête pour la page 85...\n",
      "2025-12-16 20:36:31,450 - INFO - Page 85 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:31,451 - INFO - Requête pour la page 86...\n",
      "2025-12-16 20:36:33,400 - INFO - Page 86 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:33,400 - INFO - Requête pour la page 87...\n",
      "2025-12-16 20:36:34,490 - INFO - Page 87 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:34,490 - INFO - Requête pour la page 88...\n",
      "2025-12-16 20:36:35,931 - INFO - Page 88 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:35,932 - INFO - Requête pour la page 89...\n",
      "2025-12-16 20:36:37,120 - INFO - Page 89 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:37,121 - INFO - Requête pour la page 90...\n",
      "2025-12-16 20:36:38,324 - INFO - Page 90 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:38,325 - INFO - Requête pour la page 91...\n",
      "2025-12-16 20:36:39,708 - INFO - Page 91 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:39,709 - INFO - Requête pour la page 92...\n",
      "2025-12-16 20:36:40,885 - INFO - Page 92 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:40,886 - INFO - Requête pour la page 93...\n",
      "2025-12-16 20:36:42,061 - INFO - Page 93 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:42,062 - INFO - Requête pour la page 94...\n",
      "2025-12-16 20:36:43,153 - INFO - Page 94 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:43,154 - INFO - Requête pour la page 95...\n",
      "2025-12-16 20:36:44,261 - INFO - Page 95 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:44,261 - INFO - Requête pour la page 96...\n",
      "2025-12-16 20:36:45,658 - INFO - Page 96 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:45,658 - INFO - Requête pour la page 97...\n",
      "2025-12-16 20:36:46,896 - INFO - Page 97 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:46,896 - INFO - Requête pour la page 98...\n",
      "2025-12-16 20:36:48,100 - INFO - Page 98 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:48,101 - INFO - Requête pour la page 99...\n",
      "2025-12-16 20:36:49,224 - INFO - Page 99 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:49,224 - INFO - Requête pour la page 100...\n",
      "2025-12-16 20:36:50,332 - INFO - Page 100 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:50,332 - INFO - Requête pour la page 101...\n",
      "2025-12-16 20:36:51,769 - INFO - Page 101 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:51,770 - INFO - Requête pour la page 102...\n",
      "2025-12-16 20:36:53,442 - INFO - Page 102 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:53,443 - INFO - Requête pour la page 103...\n",
      "2025-12-16 20:36:55,606 - INFO - Page 103 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:55,607 - INFO - Requête pour la page 104...\n",
      "2025-12-16 20:36:56,950 - INFO - Page 104 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:56,950 - INFO - Requête pour la page 105...\n",
      "2025-12-16 20:36:58,175 - INFO - Page 105 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:58,176 - INFO - Requête pour la page 106...\n",
      "2025-12-16 20:36:59,299 - INFO - Page 106 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:36:59,300 - INFO - Requête pour la page 107...\n",
      "2025-12-16 20:37:00,366 - INFO - Page 107 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:00,367 - INFO - Requête pour la page 108...\n",
      "2025-12-16 20:37:01,525 - INFO - Page 108 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:01,525 - INFO - Requête pour la page 109...\n",
      "2025-12-16 20:37:02,657 - INFO - Page 109 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:02,658 - INFO - Requête pour la page 110...\n",
      "2025-12-16 20:37:03,870 - INFO - Page 110 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:03,872 - INFO - Requête pour la page 111...\n",
      "2025-12-16 20:37:05,410 - INFO - Page 111 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:05,410 - INFO - Requête pour la page 112...\n",
      "2025-12-16 20:37:07,783 - INFO - Page 112 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:07,784 - INFO - Requête pour la page 113...\n",
      "2025-12-16 20:37:09,037 - INFO - Page 113 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:09,038 - INFO - Requête pour la page 114...\n",
      "2025-12-16 20:37:10,207 - INFO - Page 114 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:10,208 - INFO - Requête pour la page 115...\n",
      "2025-12-16 20:37:11,412 - INFO - Page 115 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:11,412 - INFO - Requête pour la page 116...\n",
      "2025-12-16 20:37:12,730 - INFO - Page 116 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:12,732 - INFO - Requête pour la page 117...\n",
      "2025-12-16 20:37:14,065 - INFO - Page 117 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:14,065 - INFO - Requête pour la page 118...\n",
      "2025-12-16 20:37:15,731 - INFO - Page 118 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:15,731 - INFO - Requête pour la page 119...\n",
      "2025-12-16 20:37:17,303 - INFO - Page 119 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:17,303 - INFO - Requête pour la page 120...\n",
      "2025-12-16 20:37:18,497 - INFO - Page 120 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:18,497 - INFO - Requête pour la page 121...\n",
      "2025-12-16 20:37:20,155 - INFO - Page 121 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:20,156 - INFO - Requête pour la page 122...\n",
      "2025-12-16 20:37:21,583 - INFO - Page 122 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:21,583 - INFO - Requête pour la page 123...\n",
      "2025-12-16 20:37:22,741 - INFO - Page 123 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:22,742 - INFO - Requête pour la page 124...\n",
      "2025-12-16 20:37:24,038 - INFO - Page 124 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:24,039 - INFO - Requête pour la page 125...\n",
      "2025-12-16 20:37:27,303 - INFO - Page 125 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:27,304 - INFO - Requête pour la page 126...\n",
      "2025-12-16 20:37:29,733 - INFO - Page 126 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:29,733 - INFO - Requête pour la page 127...\n",
      "2025-12-16 20:37:30,905 - INFO - Page 127 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:30,906 - INFO - Requête pour la page 128...\n",
      "2025-12-16 20:37:32,540 - INFO - Page 128 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:32,541 - INFO - Requête pour la page 129...\n",
      "2025-12-16 20:37:33,644 - INFO - Page 129 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:33,645 - INFO - Requête pour la page 130...\n",
      "2025-12-16 20:37:34,810 - INFO - Page 130 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:34,811 - INFO - Requête pour la page 131...\n",
      "2025-12-16 20:37:35,997 - INFO - Page 131 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:35,998 - INFO - Requête pour la page 132...\n",
      "2025-12-16 20:37:37,529 - INFO - Page 132 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:37,529 - INFO - Requête pour la page 133...\n",
      "2025-12-16 20:37:38,644 - INFO - Page 133 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:38,645 - INFO - Requête pour la page 134...\n",
      "2025-12-16 20:37:39,709 - INFO - Page 134 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:39,709 - INFO - Requête pour la page 135...\n",
      "2025-12-16 20:37:40,904 - INFO - Page 135 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:40,904 - INFO - Requête pour la page 136...\n",
      "2025-12-16 20:37:42,079 - INFO - Page 136 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:42,080 - INFO - Requête pour la page 137...\n",
      "2025-12-16 20:37:43,120 - INFO - Page 137 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:43,121 - INFO - Requête pour la page 138...\n",
      "2025-12-16 20:37:44,163 - INFO - Page 138 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:44,165 - INFO - Requête pour la page 139...\n",
      "2025-12-16 20:37:45,274 - INFO - Page 139 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:45,275 - INFO - Requête pour la page 140...\n",
      "2025-12-16 20:37:46,494 - INFO - Page 140 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:46,494 - INFO - Requête pour la page 141...\n",
      "2025-12-16 20:37:47,839 - INFO - Page 141 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:47,840 - INFO - Requête pour la page 142...\n",
      "2025-12-16 20:37:49,203 - INFO - Page 142 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:49,207 - INFO - Requête pour la page 143...\n",
      "2025-12-16 20:37:50,674 - INFO - Page 143 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:50,675 - INFO - Requête pour la page 144...\n",
      "2025-12-16 20:37:55,770 - INFO - Page 144 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:55,771 - INFO - Requête pour la page 145...\n",
      "2025-12-16 20:37:57,692 - INFO - Page 145 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:57,693 - INFO - Requête pour la page 146...\n",
      "2025-12-16 20:37:58,771 - INFO - Page 146 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:58,772 - INFO - Requête pour la page 147...\n",
      "2025-12-16 20:37:59,930 - INFO - Page 147 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:37:59,931 - INFO - Requête pour la page 148...\n",
      "2025-12-16 20:38:01,084 - INFO - Page 148 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:01,085 - INFO - Requête pour la page 149...\n",
      "2025-12-16 20:38:02,365 - INFO - Page 149 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:02,365 - INFO - Requête pour la page 150...\n",
      "2025-12-16 20:38:03,606 - INFO - Page 150 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:03,607 - INFO - Requête pour la page 151...\n",
      "2025-12-16 20:38:04,640 - INFO - Page 151 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:04,641 - INFO - Requête pour la page 152...\n",
      "2025-12-16 20:38:06,002 - INFO - Page 152 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:06,002 - INFO - Requête pour la page 153...\n",
      "2025-12-16 20:38:07,226 - INFO - Page 153 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:07,226 - INFO - Requête pour la page 154...\n",
      "2025-12-16 20:38:08,403 - INFO - Page 154 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:08,403 - INFO - Requête pour la page 155...\n",
      "2025-12-16 20:38:11,620 - INFO - Page 155 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:11,621 - INFO - Requête pour la page 156...\n",
      "2025-12-16 20:38:12,764 - INFO - Page 156 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:12,765 - INFO - Requête pour la page 157...\n",
      "2025-12-16 20:38:13,916 - INFO - Page 157 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:13,917 - INFO - Requête pour la page 158...\n",
      "2025-12-16 20:38:15,046 - INFO - Page 158 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:15,047 - INFO - Requête pour la page 159...\n",
      "2025-12-16 20:38:16,245 - INFO - Page 159 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:16,246 - INFO - Requête pour la page 160...\n",
      "2025-12-16 20:38:17,522 - INFO - Page 160 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:17,522 - INFO - Requête pour la page 161...\n",
      "2025-12-16 20:38:18,642 - INFO - Page 161 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:18,643 - INFO - Requête pour la page 162...\n",
      "2025-12-16 20:38:19,764 - INFO - Page 162 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:19,765 - INFO - Requête pour la page 163...\n",
      "2025-12-16 20:38:21,149 - INFO - Page 163 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:21,150 - INFO - Requête pour la page 164...\n",
      "2025-12-16 20:38:23,791 - INFO - Page 164 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:23,792 - INFO - Requête pour la page 165...\n",
      "2025-12-16 20:38:25,090 - INFO - Page 165 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:25,090 - INFO - Requête pour la page 166...\n",
      "2025-12-16 20:38:26,490 - INFO - Page 166 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:26,490 - INFO - Requête pour la page 167...\n",
      "2025-12-16 20:38:27,990 - INFO - Page 167 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:27,990 - INFO - Requête pour la page 168...\n",
      "2025-12-16 20:38:29,586 - INFO - Page 168 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:29,587 - INFO - Requête pour la page 169...\n",
      "2025-12-16 20:38:30,983 - INFO - Page 169 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:30,984 - INFO - Requête pour la page 170...\n",
      "2025-12-16 20:38:32,401 - INFO - Page 170 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:32,402 - INFO - Requête pour la page 171...\n",
      "2025-12-16 20:38:33,684 - INFO - Page 171 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:33,684 - INFO - Requête pour la page 172...\n",
      "2025-12-16 20:38:34,805 - INFO - Page 172 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:34,805 - INFO - Requête pour la page 173...\n",
      "2025-12-16 20:38:36,096 - INFO - Page 173 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:36,097 - INFO - Requête pour la page 174...\n",
      "2025-12-16 20:38:37,689 - INFO - Page 174 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:37,689 - INFO - Requête pour la page 175...\n",
      "2025-12-16 20:38:39,160 - INFO - Page 175 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:39,160 - INFO - Requête pour la page 176...\n",
      "2025-12-16 20:38:40,252 - INFO - Page 176 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:40,253 - INFO - Requête pour la page 177...\n",
      "2025-12-16 20:38:42,000 - INFO - Page 177 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:42,000 - INFO - Requête pour la page 178...\n",
      "2025-12-16 20:38:43,169 - INFO - Page 178 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:43,170 - INFO - Requête pour la page 179...\n",
      "2025-12-16 20:38:44,382 - INFO - Page 179 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:44,382 - INFO - Requête pour la page 180...\n",
      "2025-12-16 20:38:45,537 - INFO - Page 180 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:45,538 - INFO - Requête pour la page 181...\n",
      "2025-12-16 20:38:46,665 - INFO - Page 181 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:46,665 - INFO - Requête pour la page 182...\n",
      "2025-12-16 20:38:47,791 - INFO - Page 182 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:47,792 - INFO - Requête pour la page 183...\n",
      "2025-12-16 20:38:48,981 - INFO - Page 183 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:48,982 - INFO - Requête pour la page 184...\n",
      "2025-12-16 20:38:50,373 - INFO - Page 184 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:50,374 - INFO - Requête pour la page 185...\n",
      "2025-12-16 20:38:51,752 - INFO - Page 185 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:51,753 - INFO - Requête pour la page 186...\n",
      "2025-12-16 20:38:52,949 - INFO - Page 186 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:52,949 - INFO - Requête pour la page 187...\n",
      "2025-12-16 20:38:54,052 - INFO - Page 187 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:54,052 - INFO - Requête pour la page 188...\n",
      "2025-12-16 20:38:55,156 - INFO - Page 188 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:55,157 - INFO - Requête pour la page 189...\n",
      "2025-12-16 20:38:56,435 - INFO - Page 189 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:56,436 - INFO - Requête pour la page 190...\n",
      "2025-12-16 20:38:57,686 - INFO - Page 190 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:57,687 - INFO - Requête pour la page 191...\n",
      "2025-12-16 20:38:58,877 - INFO - Page 191 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:58,878 - INFO - Requête pour la page 192...\n",
      "2025-12-16 20:38:59,970 - INFO - Page 192 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:38:59,971 - INFO - Requête pour la page 193...\n",
      "2025-12-16 20:39:01,329 - INFO - Page 193 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:39:01,330 - INFO - Requête pour la page 194...\n",
      "2025-12-16 20:39:02,640 - INFO - Page 194 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:39:02,641 - INFO - Requête pour la page 195...\n",
      "2025-12-16 20:39:05,415 - INFO - Page 195 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:39:05,416 - INFO - Requête pour la page 196...\n",
      "2025-12-16 20:39:07,017 - INFO - Page 196 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:39:07,018 - INFO - Requête pour la page 197...\n",
      "2025-12-16 20:39:09,432 - INFO - Page 197 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:39:09,433 - INFO - Requête pour la page 198...\n",
      "2025-12-16 20:39:10,953 - INFO - Page 198 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:39:10,954 - INFO - Requête pour la page 199...\n",
      "2025-12-16 20:39:12,398 - INFO - Page 199 récupérée avec succès. Nombre de produits: 100\n",
      "2025-12-16 20:39:12,399 - INFO - Requête pour la page 200...\n",
      "2025-12-16 20:39:13,530 - INFO - Page 200 récupérée avec succès. Nombre de produits: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici les 5 premiers produits récupérés :\n",
      "\n",
      "--- Produit 1 ---\n",
      "code: 7622210449283\n",
      "product_name: Prince Goût Chocolat\n",
      "brands: LU\n",
      "categories: Snacks,Breakfasts,Sweet snacks,Biscuits and cakes,Biscuits and crackers,Sandwich biscuits\n",
      "nutriscore_grade: e\n",
      "energy_100g: 1960\n",
      "fat_100g: 17.5\n",
      "sugars_100g: 31.5\n",
      "salt_100g: 0.5\n",
      "proteins_100g: 6.5\n",
      "\n",
      "--- Produit 2 ---\n",
      "code: 3046920029759\n",
      "product_name: Excellence Noir Prodigieux 90% Cacao\n",
      "brands: Lindt\n",
      "categories: Snacks,Snacks sucrés,Cacao et dérivés,Chocolats,Chocolats noirs,Chocolats noirs en tablette,Chocolats noirs extra fin\n",
      "nutriscore_grade: e\n",
      "energy_100g: 2477\n",
      "fat_100g: 55\n",
      "sugars_100g: 7\n",
      "salt_100g: 0.03\n",
      "proteins_100g: 10\n",
      "\n",
      "--- Produit 3 ---\n",
      "code: 3017620425035\n",
      "product_name: Nutella\n",
      "brands: Ferrero\n",
      "categories: Petit-déjeuners,Produits à tartiner,Produits à tartiner sucrés,Pâtes à tartiner,Pâtes à tartiner au chocolat,Pâtes à tartiner aux noisettes,Pâtes à tartiner aux noisettes et au cacao\n",
      "nutriscore_grade: e\n",
      "energy_100g: 2252\n",
      "fat_100g: 30.9\n",
      "sugars_100g: 56.3\n",
      "salt_100g: 0.107\n",
      "proteins_100g: 6.3\n",
      "\n",
      "--- Produit 4 ---\n",
      "code: 20995553\n",
      "product_name: Chocolat noir - 85% cacao\n",
      "brands: J. D. Gross\n",
      "categories: Snacks,Sweet snacks,Cocoa and its products,Chocolates,Dark chocolates\n",
      "nutriscore_grade: d\n",
      "energy_100g: 2510\n",
      "fat_100g: 48\n",
      "sugars_100g: 12\n",
      "salt_100g: 0\n",
      "proteins_100g: 12\n",
      "\n",
      "--- Produit 5 ---\n",
      "code: 7622210578464\n",
      "product_name: Organic 70% Dark Chocolate Bar\n",
      "brands: Green & Black's\n",
      "categories: Snacks,Snacks sucrés,Cacao et dérivés,Chocolats,Chocolats noirs,Chocolat noir en tablette extra dégustation à 70% de cacao minimum\n",
      "nutriscore_grade: e\n",
      "energy_100g: 2428\n",
      "fat_100g: 42\n",
      "sugars_100g: 29\n",
      "salt_100g: 0.01\n",
      "proteins_100g: 9.1\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import logging\n",
    "import nest_asyncio\n",
    "\n",
    "# Permet de réutiliser la boucle asyncio existante dans Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# URL de base de l'API OpenFoodFacts\n",
    "BASE_URL = \"https://world.openfoodfacts.org/cgi/search.pl\"\n",
    "\n",
    "# Paramètres de recherche\n",
    "SEARCH_PARAMS = {\n",
    "    \"search_terms\": \"chocolat\",\n",
    "    \"search_simple\": 1,\n",
    "    \"action\": \"process\",\n",
    "    \"json\": 1,\n",
    "    \"page_size\": 100,\n",
    "}\n",
    "\n",
    "# Champs spécifiques à récupérer\n",
    "FIELDS_TO_FETCH = \"code,product_name,brands,categories,nutriscore_grade,energy_100g,fat_100g,sugars_100g,salt_100g,proteins_100g\"\n",
    "\n",
    "# Configuration du retry\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY_SECONDS = 5\n",
    "\n",
    "\n",
    "MAX_PAGES = 200  # Limite à 200 pages\n",
    "\n",
    "async def fetch_products_paginated():\n",
    "    all_products = []\n",
    "    page = 1\n",
    "    logging.info(f\"Début de la récupération des produits. Terme de recherche: '{SEARCH_PARAMS.get('search_terms')}'\")\n",
    "\n",
    "    while page <= MAX_PAGES:  # <-- limite à 200 pages\n",
    "        current_params = SEARCH_PARAMS.copy()\n",
    "        current_params[\"page\"] = page\n",
    "        current_params[\"fields\"] = FIELDS_TO_FETCH\n",
    "\n",
    "        retries = 0\n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                logging.info(f\"Requête pour la page {page}...\")\n",
    "                async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "                    response = await client.get(BASE_URL, params=current_params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "\n",
    "                if not data.get(\"products\"):\n",
    "                    logging.info(f\"Plus de produits trouvés ou fin de la pagination après la page {page-1}.\")\n",
    "                    return all_products\n",
    "\n",
    "                logging.info(f\"Page {page} récupérée avec succès. Nombre de produits: {len(data['products'])}\")\n",
    "                all_products.extend(data[\"products\"])\n",
    "                page += 1\n",
    "                break  # Sortir de la boucle retry\n",
    "\n",
    "            except (httpx.RequestError, httpx.HTTPStatusError) as exc:\n",
    "                retries += 1\n",
    "                logging.error(f\"Erreur pour la page {page}: {exc}\")\n",
    "                if retries >= MAX_RETRIES:\n",
    "                    logging.error(f\"Nombre maximal de tentatives atteint pour la page {page}. Abandon.\")\n",
    "                    return all_products\n",
    "                logging.info(f\"Nouvelle tentative dans {RETRY_DELAY_SECONDS} secondes...\")\n",
    "                await asyncio.sleep(RETRY_DELAY_SECONDS)\n",
    "\n",
    "            except Exception as exc:\n",
    "                retries += 1\n",
    "                logging.error(f\"Erreur inattendue pour la page {page}: {exc}\")\n",
    "                if retries >= MAX_RETRIES:\n",
    "                    logging.error(f\"Nombre maximal de tentatives atteint pour la page {page}. Abandon.\")\n",
    "                    return all_products\n",
    "                await asyncio.sleep(RETRY_DELAY_SECONDS)\n",
    "\n",
    "    return all_products\n",
    "\n",
    "\n",
    "\n",
    "# Exemple d'utilisation directement dans Jupyter\n",
    "products = await fetch_products_paginated()\n",
    "\n",
    "if products:\n",
    "    print(f\"Voici les {min(5, len(products))} premiers produits récupérés :\")\n",
    "    for i, product in enumerate(products[:5]):\n",
    "        print(f\"\\n--- Produit {i+1} ---\")\n",
    "        for field in FIELDS_TO_FETCH.split(','):\n",
    "            print(f\"{field}: {product.get(field, 'N/A')}\")\n",
    "else:\n",
    "    print(\"Aucun produit n'a été récupéré.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbdcf9c",
   "metadata": {},
   "source": [
    "La récupération des produits a été effectuée avec succès pour le terme de recherche \"chocolat\", couvrant un total de 200 pages, chaque page contenant 100 produits, soit un total de 20 000 produits extraits. Chaque produit inclut des informations détaillées telles que le code-barres, le nom du produit, la marque, les catégories, le Nutri-Score, ainsi que les valeurs nutritionnelles pour 100 g (énergie, graisses, sucres, sel, protéines). Les premiers produits récupérés illustrent la diversité des articles, allant des biscuits chocolatés aux tablettes de chocolat noir en passant par les pâtes à tartiner, avec des profils nutritionnels variés. Ce dataset permet ainsi une analyse fine des produits chocolatés disponibles sur le marché."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077022be",
   "metadata": {},
   "source": [
    "#### 3.2 Exercice : Personnaliser le nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd700e",
   "metadata": {},
   "source": [
    "1. Exécutez `generate_cleaning_code()` avec vos données réelles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f669292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame créé : (20000, 10)\n"
     ]
    }
   ],
   "source": [
    "from pipeline.transformer import raw_to_dataframe, generate_cleaning_code\n",
    "\n",
    "df_raw = raw_to_dataframe(products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4313c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m20:39:15 - LiteLLM:INFO\u001b[0m: utils.py:3443 - \n",
      "LiteLLM completion() model= mistral; provider = ollama\n",
      "2025-12-16 20:39:15,034 - INFO - \n",
      "LiteLLM completion() model= mistral; provider = ollama\n",
      "\u001b[92m20:43:00 - LiteLLM:INFO\u001b[0m: utils.py:1311 - Wrapper: Completed Call, calling success_handler\n",
      "2025-12-16 20:43:00,427 - INFO - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ici est un exemple de code Python Pandas qui effectue la gestion des valeurs manquantes, types et normalisation pour votre DataFrame :\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Créez un DataFrame à partir de vos données\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Génération des noms de colonnes pour les types standardisés\n",
      "standardized_columns = ['brands', 'categories', 'code', 'energy_100g', 'fat_100g', 'nutriscore_grade', 'product_name', 'proteins_100g', 'salt_100g', 'sugars_100g']\n",
      "\n",
      "# Génération des noms de colonnes pour les types catégories\n",
      "categorical_columns = ['brands', 'categories', 'nutriscore_grade']\n",
      "\n",
      "# Normalisation des données numériques (StandardScaler)\n",
      "scaler = StandardScaler()\n",
      "df[standardized_columns] = scaler.fit_transform(df[standardized_columns])\n",
      "\n",
      "# Génération de la liste des valeurs manquantes par colonne\n",
      "missing_values = df.isnull().sum()\n",
      "print(\"Nombre de valeurs manquantes par colonne :\\n\", missing_values)\n",
      "\n",
      "# Remplacement des valeurs manquantes par une valeur par défaut (par exemple, la moyenne ou la médiane)\n",
      "df['energy_100g'].fillna(df['energy_100g'].mean(), inplace=True)\n",
      "df['fat_100g'].fillna(df['fat_100g'].median(), inplace=True)\n",
      "df['proteins_100g'].fillna(df['proteins_100g'].mean(), inplace=True)\n",
      "df['salt_100g'].fillna(df['salt_100g'].median(), inplace=True)\n",
      "df['sugars_100g'].fillna(df['sugars_100g'].mean(), inplace=True)\n",
      "\n",
      "# Transformation des colonnes en catégories (OneHotEncoder ou LabelEncoder)\n",
      "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
      "\n",
      "encoder = OneHotEncoder()\n",
      "df[categorical_columns] = encoder.fit_transform(df[categorical_columns])\n",
      "\n",
      "# Vérifiez la structure du DataFrame pour vérifier les modifications effectuées\n",
      "print(\"Structure du DataFrame après nettoyage :\\n\", df.info())\n",
      "```\n",
      "\n",
      "Ce code nettoie le DataFrame en remplaçant les valeurs manquantes par une moyenne ou une médiane, normalise les données numériques et transforme les colonnes catégoriques en variables numériques.\n"
     ]
    }
   ],
   "source": [
    "cleaning_code = generate_cleaning_code(df_raw)\n",
    "print(cleaning_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe51cc",
   "metadata": {},
   "source": [
    "2. Analysez le code proposé par l'IA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe70f91",
   "metadata": {},
   "source": [
    "Le code proposé par l’IA couvre les principales étapes du nettoyage de données (gestion des valeurs manquantes, normalisation, encodage). Cependant, il présente plusieurs erreurs critiques, notamment l’application de StandardScaler à des colonnes non numériques et une mauvaise utilisation de OneHotEncoder. De plus, le code mélange nettoyage générique et préparation pour le machine learning, ce qui nuit à la modularité du pipeline. Une adaptation est donc nécessaire pour garantir un code robuste, réutilisable et exécutable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d47dc",
   "metadata": {},
   "source": [
    "3. Intégrez les transformations pertinentes dans `clean_dataframe()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8ba9d",
   "metadata": {},
   "source": [
    "Clean_dataframe() modifié avec les transformations proposées par l'IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70d30d",
   "metadata": {},
   "source": [
    "4. Testez et ajustez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "676aea6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons supprimés : 0\n",
      "DataFrame nettoyé : (20000, 10)\n",
      "            brands                                         categories  \\\n",
      "0               lu  snacks,breakfasts,sweet snacks,biscuits and ca...   \n",
      "1            lindt  snacks,snacks sucrés,cacao et dérivés,chocolat...   \n",
      "2          ferrero  petit-déjeuners,produits à tartiner,produits à...   \n",
      "3      j. d. gross  snacks,sweet snacks,cocoa and its products,cho...   \n",
      "4  green & black's  snacks,snacks sucrés,cacao et dérivés,chocolat...   \n",
      "\n",
      "            code  energy_100g  fat_100g nutriscore_grade  \\\n",
      "0  7622210449283       1960.0      17.5                e   \n",
      "1  3046920029759       2477.0      55.0                e   \n",
      "2  3017620425035       2252.0      30.9                e   \n",
      "3       20995553       2510.0      48.0                d   \n",
      "4  7622210578464       2428.0      42.0                e   \n",
      "\n",
      "                           product_name  proteins_100g  salt_100g  sugars_100g  \n",
      "0                  prince goût chocolat            6.5      0.500         31.5  \n",
      "1  excellence noir prodigieux 90% cacao           10.0      0.030          7.0  \n",
      "2                               nutella            6.3      0.107         56.3  \n",
      "3             chocolat noir - 85% cacao           12.0      0.000         12.0  \n",
      "4        organic 70% dark chocolate bar            9.1      0.010         29.0  \n",
      "Données nettoyées sauvegardées : data\\processed\\chocolats_clean_20251216_215516.parquet (1.02 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data\\\\processed\\\\chocolats_clean_20251216_215516.parquet'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from pipeline.transformer import clean_dataframe, raw_to_dataframe\n",
    "from pipeline.storage import save_clean_parquet\n",
    "\n",
    "df_clean = clean_dataframe(df_raw)\n",
    "print(df_clean.head())\n",
    "\n",
    "# Sauvegarde\n",
    "save_clean_parquet(df_clean, \"chocolats_clean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a00e77",
   "metadata": {},
   "source": [
    "### Partie 4 : Stockage et orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653db64",
   "metadata": {},
   "source": [
    "##### 4.3 Exécution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959cb38",
   "metadata": {},
   "source": [
    "Création du package __init__.py\n",
    "\n",
    "Exécuter\n",
    "uv run python -m pipeline.main --category chocolats --name chocolats_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ded72",
   "metadata": {},
   "source": [
    "### Partie 5 : Vérification avec DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac13a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes numériques détectées : ['energy_100g', 'fat_100g', 'proteins_100g', 'salt_100g', 'sugars_100g']\n",
      "Moyenne de energy_100g : 1887.27\n",
      "Moyenne de fat_100g : 24.73\n",
      "Moyenne de proteins_100g : 6.99\n",
      "Moyenne de salt_100g : 0.33\n",
      "Moyenne de sugars_100g : 33.05\n",
      "\n",
      "Valeurs manquantes par colonne :\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Colonnes marques détectées : ['brands']\n",
      "\n",
      "Valeurs uniques par colonne marque :\n",
      "brands: 4146 valeurs uniques\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Vérification des données nettoyées avec DuckDB.\"\"\"\n",
    "import duckdb\n",
    "\n",
    "# Connexion\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Lire uniquement les fichiers Parquet des données nettoyées\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM read_parquet('data/processed/chocolats_clean_*.parquet')\n",
    "\"\"\").df()\n",
    "\n",
    "# Identifier les colonnes numériques\n",
    "num_cols = df.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "print(\"Colonnes numériques détectées :\", num_cols)\n",
    "\n",
    "# Statistiques globales pour toutes les colonnes numériques\n",
    "for col in num_cols:\n",
    "    avg_val = df[col].mean()\n",
    "    print(f\"Moyenne de {col} : {avg_val:.2f}\")\n",
    "\n",
    "# Valeurs manquantes par colonne\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nValeurs manquantes par colonne :\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Identifier les colonnes de marques\n",
    "brand_cols = [c for c in df.columns if \"brands\" in c.lower()]\n",
    "print(\"\\nColonnes marques détectées :\", brand_cols)\n",
    "\n",
    "# Nombre de valeurs uniques par colonne marque\n",
    "print(\"\\nValeurs uniques par colonne marque :\")\n",
    "for col in brand_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col}: {unique_count} valeurs uniques\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff38b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre de marques uniques : 4146\n",
      "brands\n",
      "non renseigné    950\n",
      "u                425\n",
      "carrefour        386\n",
      "lindt            383\n",
      "                 337\n",
      "leader price     299\n",
      "casino           269\n",
      "nestlé           242\n",
      "cora             229\n",
      "auchan           219\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "brand_col = 'brands'\n",
    "unique_brands = df[brand_col].nunique()\n",
    "print(f\"\\nNombre de marques uniques : {unique_brands}\")\n",
    "print(df[brand_col].value_counts().head(10))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce25f9ed",
   "metadata": {},
   "source": [
    "Nous avons identifié 4146 marques uniques dans la colonne brands. Parmi ces marques, certaines valeurs sont manquantes ou mal renseignées, par exemple non renseigné apparaît 950 fois et des entrées ambiguës comme u apparaissent 425 fois. Les marques les plus fréquentes sont carrefour (386 occurrences), lindt (383), une valeur vide (337), leader price (299), casino (269), nestlé (242), cora (229) et auchan (219). Cette distribution montre que, bien que le dataset soit riche en marques différentes, une proportion significative des données nécessite un nettoyage ou une standardisation supplémentaire pour éviter les doublons et homogénéiser les noms des marques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
