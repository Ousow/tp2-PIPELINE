"""Script principal du pipeline."""
import argparse
from .fetcher import fetch_all_data
from .transformer import raw_to_dataframe, clean_dataframe
from .storage import save_raw_json, save_parquet


def run_pipeline(category: str, name: str):
    """
    ExÃ©cute le pipeline complet.
    
    Args:
        category: CatÃ©gorie Ã  rÃ©cupÃ©rer
        name: Nom pour les fichiers de sortie
    """
    print("=" * 50)
    print(f"PIPELINE : {name}")
    print("=" * 50)
    
    # Ã‰tape 1 : Acquisition
    print("\nğŸ“¥ Ã‰tape 1 : Acquisition des donnÃ©es")
    raw_data = fetch_all_data(category)
    save_raw_json(raw_data, name)
    
    # Ã‰tape 2 : Transformation
    print("\nğŸ”§ Ã‰tape 2 : Transformation")
    df = raw_to_dataframe(raw_data)
    df_clean = clean_dataframe(df)
    
    # Ã‰tape 3 : Stockage
    print("\nğŸ’¾ Ã‰tape 3 : Stockage")
    output_path = save_parquet(df_clean, name)
    
    print("\n" + "=" * 50)
    print("âœ… Pipeline terminÃ© avec succÃ¨s !")
    print(f"ğŸ“ Fichier : {output_path}")
    print("=" * 50)
    
    return output_path


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipeline Open Data")
    parser.add_argument("--category", default="chocolats", help="CatÃ©gorie Ã  rÃ©cupÃ©rer")
    parser.add_argument("--name", default="products", help="Nom du dataset")
    
    args = parser.parse_args()
    run_pipeline(args.category, args.name)